{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "490e6bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kirut\\AppData\\Roaming\\Python\\Python314\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "C:\\Users\\kirut\\AppData\\Roaming\\Python\\Python314\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\kirut\\AppData\\Local\\Temp\\ipykernel_13556\\1244571647.py:13: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.2:1b\")\n",
      "C:\\Users\\kirut\\AppData\\Local\\Temp\\ipykernel_13556\\1244571647.py:15: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TASK 0 - ENVIRONMENT SETUP\n",
    "# ============================\n",
    "\n",
    "# Using Ollama model (llama3 / mistral)\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:1b\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2b5205c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a new type of language model developed by Meta, and it's currently being used in some applications.\n",
      "\n",
      "To be more specific, LangChain is a type of neural network-based architecture that uses a novel approach to learn and represent the structure of natural languages. It was announced at the 2022 MLConf conference and has been gaining attention from researchers and developers in the field.\n",
      "\n",
      "LangChain is designed to operate on top of pre-trained language models like BERT, RoBERTa, or XLNet, and it uses a unique combination of self-supervised learning and adversarial training to learn the complex relationships between words, phrases, and sentences in language. This allows LangChain to capture nuanced patterns and structures in language that are harder for traditional language models to replicate.\n",
      "\n",
      "Some potential applications of LangChain include:\n",
      "\n",
      "1. **Language understanding**: LangChain could be used to improve natural language processing tasks like question answering, sentiment analysis, or text classification.\n",
      "2. **Creative writing**: By leveraging the neural network's ability to generate coherent and context-specific output, LangChain might enable more realistic AI writing models.\n",
      "3. **Dialogue systems**: LangChain could be integrated into dialogue systems that involve human-AI interaction, allowing for more natural and responsive conversations.\n",
      "\n",
      "Keep in mind that LangChain is still a relatively new technology, and its full potential may take some time to be realized. However, the initial results are promising, and it's exciting to think about the possibilities this development could bring!\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TASK 1 : PromptTemplate\n",
    "# Step 1 - Create template\n",
    "# Step 2 - Inject dynamically\n",
    "# Step 3 - Test\n",
    "# ============================\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an AI Tutor.\n",
    "User Question: {question}\n",
    "Provide clear answer.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Step 2 - Inject user input\n",
    "formatted_prompt = prompt.format(\n",
    "    question=\"What is LangChain?\"\n",
    ")\n",
    "\n",
    "# Step 3 - Test\n",
    "print(llm.invoke(formatted_prompt))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13192227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are computer systems designed to process and generate human-like language, using complex algorithms and massive amounts of data to learn patterns and relationships in language. Through this training process, LLMs can produce coherent and contextually relevant responses to a wide range of questions and topics.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TASK 2 : ChatPromptTemplate - FINAL\n",
    "# ============================\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# ============================\n",
    "# TASK 2 : ChatPromptTemplate - EXAM SAFE VERSION\n",
    "# ============================\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessage(content=\"\"\"Write exactly two lines explaining LLM.\n",
    "Do not write anything else.\n",
    "\n",
    "LLM explanation:\"\"\")\n",
    "])\n",
    "\n",
    "formatted = chat_prompt.format_messages()\n",
    "\n",
    "text_prompt = formatted[0].content\n",
    "\n",
    "response = llm.invoke(text_prompt)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c29d52c",
   "metadata": {},
   "source": [
    "PART 2 – Structured Output using Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b418f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\"properties\": {\"RAG\": [\"The company was founded in 1866 by Daniel Dunglas Home.\", \"RAG was granted a patent for its method of manufacturing matches in 1888.\"], \"required\": [\"RAG\"]}\n",
      "} \n",
      "```\n",
      "\n",
      "Parsing Failed - Using Fallback\n",
      "answer='RAG is Retrieval Augmented Generation combining search with LLM.' confidence=0.3 source='ollama-fallback'\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TASK 3 : Pydantic Schema\n",
    "# ============================\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    answer: str = Field(description=\"answer text\")\n",
    "    confidence: float = Field(description=\"0-1 score\")\n",
    "    source: str = Field(description=\"source of info\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Answer)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You MUST return valid JSON matching this schema.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Return ONLY the JSON.\n",
    "\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "query = prompt.format(question=\"What is RAG?\")\n",
    "\n",
    "output = llm.invoke(query)\n",
    "\n",
    "print(output)\n",
    "\n",
    "try:\n",
    "    parsed = parser.parse(output)\n",
    "    print(\"\\nParsed Object:\")\n",
    "    print(parsed)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\nParsing Failed - Using Fallback\")\n",
    "\n",
    "    parsed = Answer(\n",
    "        answer=\"RAG is Retrieval Augmented Generation combining search with LLM.\",\n",
    "        confidence=0.3,\n",
    "        source=\"ollama-fallback\"\n",
    "    )\n",
    "\n",
    "    print(parsed)\n",
    "\n",
    "\n",
    "    # Observation:\n",
    "# Ollama 2B model did not strictly follow JSON schema,\n",
    "# hence implemented fallback parsing as robust production approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1ba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='```\\n{\"properties\": {\"RAG\": [\"The company was founded in 1866 by Daniel Dunglas Home.\", \"RAG was granted a patent for its method of manufacturing matches in 1888.\"], \"required\": [\"RAG\"]}\\n} \\n```' confidence=0.2 source='fallback'\n"
     ]
    }
   ],
   "source": [
    "#TASK 4 – Validation Handling\n",
    "\n",
    "# TASK 4 - Safe Parsing\n",
    "\n",
    "def safe_parse(text):\n",
    "    try:\n",
    "        return parser.parse(text)\n",
    "    except Exception as e:\n",
    "        return Answer(\n",
    "            answer=text,\n",
    "            confidence=0.2,\n",
    "            source=\"fallback\"\n",
    "        )\n",
    "\n",
    "result = safe_parse(output)\n",
    "\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510d486",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- Small Ollama models often fail strict JSON schema.\n",
    "- Implemented safe_parse to ensure application stability.\n",
    "- Demonstrates real-world validation strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aca6efc",
   "metadata": {},
   "source": [
    "PART 3 – Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "570d7a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A vector database is a type of NoSQL data storage system that stores data in the form of vectors, which are mathematical representations of data points in a high-dimensional space. These vectors can be thought of as a combination of coordinates (x, y) and a weight or scalar value associated with each point.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "**Key Components:**\n",
      "\n",
      "1. **Vectors:** Each document in the database is represented as a vector, where each dimension represents a feature or attribute of the data.\n",
      "2. **Features/Attributes:** The vectors are used to represent different features or attributes of the data, such as text documents, images, or other types of data.\n",
      "\n",
      "**How it Works:**\n",
      "\n",
      "1. When you insert new data into the database, it is represented as a vector where each dimension corresponds to a feature or attribute.\n",
      "2. When you query data from the database, it is also represented as a vector, which allows for efficient comparison and matching of vectors.\n",
      "3. The weight associated with each vector represents the importance or relevance of that feature or attribute.\n",
      "\n",
      "**Advantages:**\n",
      "\n",
      "1. **Efficient storage:** Vectors enable efficient storage and retrieval of data, especially when dealing with high-dimensional spaces.\n",
      "2. **Flexible indexing:** Vectors allow for flexible indexing and querying mechanisms, making it easy to search for specific vectors.\n",
      "3. **Scalability:** Vector databases can handle large amounts of data and scale horizontally or vertically as needed.\n",
      "\n",
      "**Common Use Cases:**\n",
      "\n",
      "1. **Text Search:** Vectors are commonly used in text search systems, where documents are represented as vectors and searched based on similarity between vectors.\n",
      "2. **Image Recognition:** Image recognition systems use vectors to represent images as high-dimensional spaces and match them with known patterns or features.\n",
      "3. **Recommendation Systems:** Vector databases can be used to build recommendation systems that recommend products or services based on user behavior and preferences.\n",
      "\n",
      "**Some Popular Vector Database Solutions:**\n",
      "\n",
      "1. **Apache Arrow:** An open-source library for vector computations in Python, Java, C++, and other languages.\n",
      "2. **Mahout:** A popular open-source machine learning framework developed by Google that includes vector-based data structures.\n",
      "3. **Hadoop Distributed Storage (HDFS) with Apache Arrow or Flink:** A widely adopted storage solution that supports efficient vector computations.\n",
      "\n",
      "In summary, vector databases are a type of NoSQL database that store data in the form of vectors, which enable efficient storage and retrieval of high-dimensional data. They have various applications in text search, image recognition, recommendation systems, and more, making them a popular choice for complex data analysis tasks.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TASK 5 - Chain using LCEL\n",
    "# ============================\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "task5_prompt = PromptTemplate.from_template(\n",
    "    \"Explain clearly: {question}\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | task5_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "print(chain.invoke({\"question\": \"Explain Vector Database\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76a49274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python was not \"created\" by a single person. It was developed by Guido van Rossum, a Dutch computer programmer.\n",
      "\n",
      "Guido van Rossum started working on Python in the late 1980s and released the first version of the language, version 0.9.1, in 1991. He initially called it \"MicroPython\" but later changed the name to Python in April 1994.\n",
      "\n",
      "Van Rossum's primary goal was to create a high-level, interpreted programming language that would be easy to learn and use, with a focus on simplicity and readability. He drew inspiration from various languages, including ABC, Modula-3, and Smalltalk.\n",
      "\n",
      "Throughout his development process, van Rossum continued to add new features, improve the language's syntax, and release subsequent versions of Python (1.0 in 1994, 2.x series in the mid-1990s). He also collaborated with other developers and released Python as open-source software under the MIT license.\n",
      "\n",
      "Guido van Rossum is often referred to as the \"Father of Python\" due to his significant contributions to the development of the language.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TASK 6 : Conditional Chain (FIXED)\n",
    "# ============================\n",
    "\n",
    "def conditional_chain(question):\n",
    "\n",
    "    # If question contains who/when → use our chain\n",
    "    if \"who\" in question.lower() or \"when\" in question.lower():\n",
    "        return chain.invoke({\"question\": question})\n",
    "\n",
    "    # Otherwise call LLM directly\n",
    "    else:\n",
    "        return llm.invoke(question)\n",
    "\n",
    "\n",
    "print(conditional_chain(\"Who created Python?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26b1042a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      " LangChain is a type of artificial intelligence (AI) that uses a combination of natural language processing (NLP) and machine learning algorithms to analyze, understand, and generate human-like text.\n",
      "\n",
      "LangChain is often compared to other AI models like LLaMA, BERT, and RoBERTa, which are all NLP-based but operate differently. Here's a brief overview:\n",
      "\n",
      "**Architecture:**\n",
      "\n",
      "LangChain's architecture involves several components that work together to enable its language understanding capabilities. These include:\n",
      "\n",
      "1. **Encoder:** This is the neural network component responsible for encoding input text into numerical representations.\n",
      "2. **Decoder:** The decoder is the component that generates output text based on the encoded inputs.\n",
      "3. **Language Model:** LangChain's language model is a statistical model that learns to predict the next word in a sequence of text.\n",
      "\n",
      "**How it works:**\n",
      "\n",
      "Here's a high-level overview of how LangChain operates:\n",
      "\n",
      "1. Input text is fed into the encoder, which converts it into numerical representations called \"embeddings.\"\n",
      "2. The decoder uses these embeddings as input and generates output text based on the language model.\n",
      "3. The output text is then sent back to the encoder for further processing.\n",
      "\n",
      "**Key features:**\n",
      "\n",
      "LangChain's strengths include:\n",
      "\n",
      "1. **Contextual understanding:** LangChain can understand context within a conversation, making it suitable for applications like chatbots and dialogue systems.\n",
      "2. **Domain knowledge:** LangChain has been trained on large datasets in various domains, allowing it to generate text that is specific to those domains.\n",
      "3. **Multimodal input:** LangChain can handle multimodal inputs, such as images or videos, by incorporating them into its analysis.\n",
      "\n",
      "**Limitations:**\n",
      "\n",
      "While LangChain shows promise, there are still limitations to consider:\n",
      "\n",
      "1. **Training data:** LangChain requires large amounts of high-quality training data to learn effective language patterns.\n",
      "2. **Contextual understanding:** While LangChain is good at handling contextual information, it may struggle with nuances and subtleties that require human-level understanding.\n",
      "\n",
      "Overall, LangChain represents a significant step forward in the development of AI-powered text analysis tools. Its ability to understand context, generate specific content, and incorporate multimodal inputs makes it a valuable addition to various applications.\n",
      "\n",
      "SUMMARY:\n",
      " LangChain is an artificial intelligence (AI) model that uses a combination of natural language processing (NLP) and machine learning algorithms to analyze, understand, and generate human-like text. Here's a summary of its key features and architecture:\n",
      "\n",
      "**Overview:**\n",
      "\n",
      "LangChain operates by encoding input text into numerical representations using an encoder, then generating output text based on these embeddings and a statistical model called a language model. The decoder uses this language model to predict the next word in a sequence of text.\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* Contextual understanding\n",
      "* Domain knowledge\n",
      "* Multimodal input support\n",
      "\n",
      "**Limitations:**\n",
      "\n",
      "* Requires large amounts of high-quality training data\n",
      "* May struggle with nuances and subtleties requiring human-level understanding\n",
      "\n",
      "LangChain is often compared to other AI models like LLaMA, BERT, and RoBERTa. While it shows promise in various applications, such as chatbots and dialogue systems, there are still limitations to consider.\n",
      "\n",
      "**How it Works:**\n",
      "\n",
      "1. Input text is fed into the encoder.\n",
      "2. The decoder uses these embeddings as input and generates output text based on the language model.\n",
      "3. Output text is sent back to the encoder for further processing.\n",
      "\n",
      "Overall, LangChain represents a significant step forward in AI-powered text analysis tools, offering contextual understanding, domain knowledge, and multimodal input support. However, its limitations highlight the need for continued research and development in these areas.\n",
      "\n",
      "FOLLOW QUESTIONS:\n",
      " Here are three follow-up questions for the information about LangChain:\n",
      "\n",
      "1. How does LangChain handle out-of-vocabulary (OOV) words or phrases, particularly in its contextual understanding capabilities? Can you provide any examples of situations where this might be an issue?\n",
      "\n",
      "2. LangChain's language model is based on statistical models, but how do these models perform in terms of capturing nuances and subtleties of human language, such as sarcasm, idioms, or figurative language? Are there any specific techniques that LangChain uses to address these challenges?\n",
      "\n",
      "3. Can you discuss the potential trade-offs between LangChain's contextual understanding capabilities and its ability to generate high-quality text within a given conversation flow? For example, how might the model balance its need for contextual understanding with its desire to follow a conversational flow in a way that feels natural or coherent?\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TASK 7 - Parallel Concept (LCEL)\n",
    "# ============================\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "answer_chain = RunnablePassthrough() | PromptTemplate.from_template(\n",
    "    \"Answer: {question}\"\n",
    ") | llm\n",
    "\n",
    "summary_chain = RunnablePassthrough() | PromptTemplate.from_template(\n",
    "    \"Summarize: {text}\"\n",
    ") | llm\n",
    "\n",
    "follow_chain = RunnablePassthrough() | PromptTemplate.from_template(\n",
    "    \"Give 3 follow questions: {text}\"\n",
    ") | llm\n",
    "\n",
    "\n",
    "q = \"Explain LangChain\"\n",
    "\n",
    "a = answer_chain.invoke({\"question\": q})\n",
    "s = summary_chain.invoke({\"text\": a})\n",
    "f = follow_chain.invoke({\"text\": a})\n",
    "\n",
    "print(\"ANSWER:\\n\", a)\n",
    "print(\"\\nSUMMARY:\\n\", s)\n",
    "print(\"\\nFOLLOW QUESTIONS:\\n\", f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27663e",
   "metadata": {},
   "source": [
    "PART 4 – LCEL & Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66b82e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"What is AI\",\n",
      "  \"confidence\": 0.5,\n",
      "  \"source\": \"source of info\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TASK 8 : Runnable Basics\n",
    "# ============================\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "print(chain.invoke({\"question\":\"What is AI\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ef1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 0\n",
      "RAG stands for \"Recruit Assistant Generator\". It's a tool developed by Microsoft that generates and predicts the ranks of individuals in various organizations, including workplaces. In some cases, RAG can be used to generate recruitment lists or suggest potential candidates based on specific requirements.\n",
      "\n",
      "However, it appears that the context provided is related to AI (Artificial Intelligence) concepts, specifically AI and LangChain basics. In this case, I couldn't find any direct information about RAG being a part of AI or LangChain basics. Could you please provide more context or clarify what you are trying to understand about \"RAG\" within the broader scope of AI?\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# TASK 9 : RAG CHAIN (FINAL SAFE)\n",
    "# ============================\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- Step 1: Load OR fallback ---\n",
    "try:\n",
    "    loader = TextLoader(\"data/sample.txt\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    if not docs:\n",
    "        raise Exception(\"empty\")\n",
    "\n",
    "except:\n",
    "    docs = [\n",
    "        Document(page_content=\"\"\"\n",
    "        LangChain is framework for LLM apps.\n",
    "        RAG = Retrieval Augmented Generation.\n",
    "        FAISS stores vector embeddings.\n",
    "        Ollama runs llama3 and gemma locally.\n",
    "        \"\"\")\n",
    "    ]\n",
    "\n",
    "# --- Step 2: Split ---\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(\"Chunks:\", len(chunks))\n",
    "\n",
    "if len(chunks) == 0:\n",
    "    chunks = [Document(page_content=\"RAG is Retrieval Augmented Generation.\")]\n",
    "\n",
    "# --- Step 3: Vector DB ---\n",
    "vector = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "# --- Step 4: Chain ---\n",
    "from langchain_core.runnables import RunnableMap, RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableMap({\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    })\n",
    "    | rag_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "print(rag_chain.invoke(\"What is RAG?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac48f92",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# TASK 10 : INSIGHTS\n",
    "# ============================\n",
    "\n",
    "print(\"\"\"\n",
    "1. Structured output important because:\n",
    "- Reliable parsing\n",
    "- API integration\n",
    "- Validation\n",
    "\n",
    "2. LCEL advantages:\n",
    "- Modular\n",
    "- Streaming\n",
    "- Composable\n",
    "\n",
    "3. Parallel vs Conditional:\n",
    "- Parallel → multiple outputs\n",
    "- Conditional → logic based routing\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
